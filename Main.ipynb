{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gothiyag/firefly-intrusion-detection/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Define your Google Drive folder URL\n",
        "folder_url = \"https://drive.google.com/drive/u/0/folders/1Dom1KFgteCQvBDoIKvQ9b1B619MUIrJk\"\n",
        "repo_url = \"https://github.com/gothiyag/firefly-intrusion-detection.git\"\n",
        "repo_dir = \"/content/firefly-intrusion-detection\"\n",
        "\n",
        "# Define file IDs (you need to find the file IDs for the two CSV files from Google Drive)\n",
        "file_id_1 = \"14RDFD50lHdug4ds-WpwtHFcGYsWQnvmm\"  # Replace with the actual file ID of the first CSV\n",
        "file_id_2 = \"1f9dZTXsxlC6a5JywkAip9ySvIO_PMhHc\"  # Replace with the actual file ID of the second CSV\n",
        "\n",
        "# Define the sample_data directory\n",
        "sample_data_dir = \"/content/sample_data\"\n",
        "\n",
        "# Clone or pull the repository\n",
        "if os.path.exists(repo_dir):\n",
        "    # If the repo exists, pull the latest changes\n",
        "    %cd {repo_dir}\n",
        "    !git pull\n",
        "else:\n",
        "    # Clone the repository if it doesn't exist\n",
        "    !git clone {repo_url}\n",
        "    %cd {repo_dir}\n",
        "\n",
        "# Download the CSV files from Google Drive\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id_1}\", os.path.join(sample_data_dir, \"IoTID20.csv\"), quiet=False)\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id_2}\", os.path.join(sample_data_dir, \"NF-BoT-IoT-v2-5%.csv\"), quiet=False)\n",
        "\n",
        "print(\"Files have been downloaded and saved to 'sample_data' directory.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HMCTbEbudcZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt user for Git username and email\n",
        "username = input(\"Enter your GitHub username: \")\n",
        "email = input(\"Enter your GitHub email: \")\n",
        "\n",
        "# Configure Git locally for this session only\n",
        "!git config user.name \"{username}\"\n",
        "!git config user.email \"{email}\"\n",
        "\n",
        "print(\"Git configured for this session.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaAVaFCldiij",
        "outputId": "bb34d50c-f365-4ff3-a747-56bc0eb55e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub username: gothiyag\n",
            "Enter your GitHub email: gothiyag@cisco.com\n",
            "Git configured for this session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing Stage\n",
        "(Data Cleansing, Normalization and Encoding)"
      ],
      "metadata": {
        "id": "Tewf2JydGsVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('NF-BoT-IoT-v2-5%.csv')\n",
        "\n",
        "\n",
        "# Display the first few rows of the dataset to verify loading\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "NSTdoCfTHJ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleansing - Handling Missing value, Remove duplicate column"
      ],
      "metadata": {
        "id": "7xoqPOi4II-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
        "\n",
        "# Fill missing values (if any)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Remove duplicates if any\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Check dataset shape after cleansing\n",
        "print(\"Dataset shape after cleansing:\", df.shape)\n"
      ],
      "metadata": {
        "id": "kN_opLkiITqP",
        "outputId": "b1939e28-07fc-4151-cfe7-feabb5a56c76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column:\n",
            " Flow_ID     0\n",
            "Src_IP      0\n",
            "Src_Port    0\n",
            "Dst_IP      0\n",
            "Dst_Port    0\n",
            "           ..\n",
            "Idle_Max    0\n",
            "Idle_Min    0\n",
            "Label       0\n",
            "Cat         0\n",
            "Sub_Cat     0\n",
            "Length: 86, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Normalization"
      ],
      "metadata": {
        "id": "LXpubMrBI_jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Selecting numerical columns for normalization\n",
        "numeric_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply MinMax scaling\n",
        "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
        "\n",
        "# Print a summary to verify normalization\n",
        "print(\"Normalized data sample:\\n\", df.head())\n"
      ],
      "metadata": {
        "id": "9aLHMGplJJHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding and Split"
      ],
      "metadata": {
        "id": "6su5YYH8EoX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical features\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Display a sample to verify encoding\n",
        "print(\"Data after encoding:\\n\", df.head())\n"
      ],
      "metadata": {
        "id": "ppYUyUfiEs_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 'Attack' is the target variable\n",
        "X = df.drop('Attack', axis = 1)\n",
        "y = df['Attack']\n",
        "\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Check the shapes of the resulting splits\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "kG_HIoPvE_gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection\n",
        "\n",
        "\n",
        "Spearman Rank Correlation"
      ],
      "metadata": {
        "id": "aZH3Pnd3FIZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Calculate Spearman rank correlation for features with the target\n",
        "spearman_corr = X_train.corrwith(y_train, method='spearman').abs()  # Take absolute values for feature ranking\n",
        "spearman_selected_features = spearman_corr[spearman_corr > 0.2].index  # Select features above a threshold\n",
        "\n",
        "# Display selected features based on Spearman correlation\n",
        "print(\"Selected features from Spearman correlation:\\n\", spearman_selected_features)\n"
      ],
      "metadata": {
        "id": "2v34pcH8FTnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutual Information Feature Selection\n"
      ],
      "metadata": {
        "id": "Kcjg-Y9NFrfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Calculate mutual information for each feature in relation to the target\n",
        "mutual_info = mutual_info_classif(X_train, y_train)\n",
        "mutual_info_series = pd.Series(mutual_info, index=X_train.columns)\n",
        "\n",
        "# Set a threshold to select features with significant mutual information\n",
        "mutual_info_selected_features = mutual_info_series[mutual_info_series > 0.1].index\n",
        "\n",
        "# Display selected features based on Mutual Information\n",
        "print(\"Selected features from Mutual Information:\\n\", mutual_info_selected_features)\n"
      ],
      "metadata": {
        "id": "Gso9mYhZFv8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firefly Algorithm"
      ],
      "metadata": {
        "id": "XowfoT5MF1WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parameters for Firefly Algorithm\n",
        "num_fireflies = 20\n",
        "max_iterations = 50\n",
        "gamma = 1.0  # Absorption coefficient\n",
        "alpha = 0.2  # Randomness factor\n",
        "\n",
        "# Scoring function that combines Spearman and Mutual Information scores\n",
        "# Normalize scores between 0 and 1, then sum\n",
        "spearman_scores = spearman_corr / spearman_corr.max()\n",
        "mutual_info_scores = mutual_info_series / mutual_info_series.max()\n",
        "combined_scores = spearman_scores + mutual_info_scores\n",
        "\n",
        "# Initialize fireflies with random subsets of features\n",
        "np.random.seed(42)\n",
        "fireflies = [np.random.choice([0, 1], len(X_train.columns)) for _ in range(num_fireflies)]\n",
        "\n",
        "# Define a function to evaluate firefly's fitness based on selected features\n",
        "def evaluate_fitness(firefly):\n",
        "    selected_features = X_train.columns[firefly == 1]\n",
        "    return combined_scores[selected_features].sum()\n",
        "\n",
        "# Firefly Optimization Loop\n",
        "for iteration in range(max_iterations):\n",
        "    for i in range(num_fireflies):\n",
        "        for j in range(num_fireflies):\n",
        "            if evaluate_fitness(fireflies[j]) > evaluate_fitness(fireflies[i]):\n",
        "                # Update firefly i towards firefly j\n",
        "                fireflies[i] = np.where(\n",
        "                    np.random.rand(len(X_train.columns)) < alpha * np.exp(-gamma * np.sum((fireflies[i] - fireflies[j]) ** 2)),\n",
        "                    fireflies[j],\n",
        "                    fireflies[i]\n",
        "                )\n",
        "\n",
        "    # Optional: Decrease alpha over iterations for reduced randomness\n",
        "    # alpha *= 0.95\n",
        "\n",
        "# Select the best firefly as the optimized feature subset\n",
        "best_firefly = max(fireflies, key=evaluate_fitness)\n",
        "optimized_features = X_train.columns[best_firefly == 1]\n",
        "\n",
        "print(\"Optimized feature subset:\\n\", optimized_features)\n",
        "\n",
        "\n",
        "# Output the final selected features\n",
        "print(\"Final selected features after Firefly Optimization:\\n\", optimized_features)\n"
      ],
      "metadata": {
        "id": "a-YYUX6qF81Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Model Building"
      ],
      "metadata": {
        "id": "npfLzJQaH5KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the data for the optimized features\n",
        "X_optimized = X_train[optimized_features]  # Select the columns from X_train that are in the optimized_features\n",
        "X_optimized_test = X_test[optimized_features]  # Select the columns from X_test that are in the optimized_features\n",
        "\n",
        "# Normalize the selected features using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_optimized = scaler.fit_transform(X_optimized)\n",
        "X_optimized_test = scaler.transform(X_optimized_test)\n",
        "\n",
        "# Reshaping the data for LSTM: LSTM expects input to be 3D (samples, timesteps, features)\n",
        "# In this case, we have 1 timestep, and the number of features corresponds to the number of optimized features\n",
        "X_optimized = X_optimized.reshape((X_optimized.shape[0], 1, X_optimized.shape[1]))\n",
        "X_optimized_test = X_optimized_test.reshape((X_optimized_test.shape[0], 1, X_optimized_test.shape[1]))\n",
        "\n",
        "# Building the LSTM model\n",
        "from tensorflow.keras.model import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Initialize the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the LSTM layer\n",
        "model.add(LSTM(units=64, return_sequences=False, input_shape=(X_optimized.shape[1], X_optimized.shape[2])))  # Number of features is the 3rd dimension of the input\n",
        "model.add(Dropout(0.2))  # Dropout layer to avoid overfitting\n",
        "\n",
        "# Add the output layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid for binary classification (attack or no attack)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_optimized, y_train, epochs=10, batch_size=64, validation_data=(X_optimized_test, y_test))\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = model.predict(X_optimized_test)\n",
        "\n",
        "# Convert predictions from probabilities to binary values (0 or 1)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model performance using accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "NAp81Qe2HsPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Validation"
      ],
      "metadata": {
        "id": "nhgjVY8CNLjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Validation: Evaluate the performance using additional metrics\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xlabel('Predicted label')\n",
        "ax.set_ylabel('True label')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ROC Curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random classifier\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Optionally, you can also include F1-Score or Precision-Recall curve if you find it necessary.\n"
      ],
      "metadata": {
        "id": "Q5naqrjQNHmf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}