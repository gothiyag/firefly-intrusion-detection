{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gothiyag/firefly-intrusion-detection/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Define your Google Drive folder URL\n",
        "folder_url = \"https://drive.google.com/drive/u/0/folders/1Dom1KFgteCQvBDoIKvQ9b1B619MUIrJk\"\n",
        "repo_url = \"https://github.com/gothiyag/firefly-intrusion-detection.git\"\n",
        "repo_dir = \"/content/firefly-intrusion-detection\"\n",
        "\n",
        "# Define file IDs (you need to find the file IDs for the two CSV files from Google Drive)\n",
        "file_id_1 = \"14RDFD50lHdug4ds-WpwtHFcGYsWQnvmm\"  # Replace with the actual file ID of the first CSV\n",
        "file_id_2 = \"1f9dZTXsxlC6a5JywkAip9ySvIO_PMhHc\"  # Replace with the actual file ID of the second CSV\n",
        "\n",
        "# Define the sample_data directory\n",
        "sample_data_dir = \"/content/sample_data\"\n",
        "\n",
        "# Clone or pull the repository\n",
        "if os.path.exists(repo_dir):\n",
        "    # If the repo exists, pull the latest changes\n",
        "    %cd {repo_dir}\n",
        "    !git pull\n",
        "else:\n",
        "    # Clone the repository if it doesn't exist\n",
        "    !git clone {repo_url}\n",
        "    %cd {repo_dir}\n",
        "\n",
        "# Download the CSV files from Google Drive\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id_1}\", os.path.join(sample_data_dir, \"IoTID20.csv\"), quiet=False)\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id_2}\", os.path.join(sample_data_dir, \"NF-BoT-IoT-v2-5%.csv\"), quiet=False)\n",
        "\n",
        "print(\"Files have been downloaded and saved to 'sample_data' directory.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HMCTbEbudcZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt user for Git username and email\n",
        "username = input(\"Enter your GitHub username: \")\n",
        "email = input(\"Enter your GitHub email: \")\n",
        "\n",
        "# Configure Git locally for this session only\n",
        "!git config user.name \"{username}\"\n",
        "!git config user.email \"{email}\"\n",
        "\n",
        "print(\"Git configured for this session.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaAVaFCldiij",
        "outputId": "bb34d50c-f365-4ff3-a747-56bc0eb55e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub username: gothiyag\n",
            "Enter your GitHub email: gothiyag@cisco.com\n",
            "Git configured for this session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing Stage\n",
        "(Data Cleansing, Normalization and Encoding)"
      ],
      "metadata": {
        "id": "Tewf2JydGsVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('NF-BoT-IoT-v2-5%.csv')\n",
        "\n",
        "\n",
        "# Display the first few rows of the dataset to verify loading\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "NSTdoCfTHJ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleansing - Handling Missing value, Remove duplicate column"
      ],
      "metadata": {
        "id": "7xoqPOi4II-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
        "\n",
        "# Fill missing values (if any)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Remove duplicates if any\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Check dataset shape after cleansing\n",
        "print(\"Dataset shape after cleansing:\", df.shape)\n"
      ],
      "metadata": {
        "id": "kN_opLkiITqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Normalization"
      ],
      "metadata": {
        "id": "LXpubMrBI_jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Selecting numerical columns for normalization\n",
        "numeric_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply MinMax scaling\n",
        "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
        "\n",
        "# Print a summary to verify normalization\n",
        "print(\"Normalized data sample:\\n\", df.head())\n"
      ],
      "metadata": {
        "id": "9aLHMGplJJHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding and Split"
      ],
      "metadata": {
        "id": "6su5YYH8EoX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical features\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Display a sample to verify encoding\n",
        "print(\"Data after encoding:\\n\", df.head())\n"
      ],
      "metadata": {
        "id": "ppYUyUfiEs_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 'Attack' is the target variable\n",
        "X = df.drop('Attack', axis = 1)\n",
        "y = df['Attack']\n",
        "\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Check the shapes of the resulting splits\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "kG_HIoPvE_gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection\n",
        "\n",
        "\n",
        "Spearman Rank Correlation"
      ],
      "metadata": {
        "id": "aZH3Pnd3FIZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Calculate Spearman rank correlation for features with the target\n",
        "spearman_corr = X_train.corrwith(y_train, method='spearman').abs()  # Take absolute values for feature ranking\n",
        "spearman_selected_features = spearman_corr[spearman_corr > 0.2].index  # Select features above a threshold\n",
        "\n",
        "# Display selected features based on Spearman correlation\n",
        "print(\"Selected features from Spearman correlation:\\n\", spearman_selected_features)\n"
      ],
      "metadata": {
        "id": "2v34pcH8FTnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutual Information Feature Selection\n"
      ],
      "metadata": {
        "id": "Kcjg-Y9NFrfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Calculate mutual information for each feature in relation to the target\n",
        "mutual_info = mutual_info_classif(X_train, y_train)\n",
        "mutual_info_series = pd.Series(mutual_info, index=X_train.columns)\n",
        "\n",
        "# Set a threshold to select features with significant mutual information\n",
        "mutual_info_selected_features = mutual_info_series[mutual_info_series > 0.1].index\n",
        "\n",
        "# Display selected features based on Mutual Information\n",
        "print(\"Selected features from Mutual Information:\\n\", mutual_info_selected_features)\n"
      ],
      "metadata": {
        "id": "Gso9mYhZFv8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firefly Algorithm"
      ],
      "metadata": {
        "id": "XowfoT5MF1WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parameters for Firefly Algorithm\n",
        "num_fireflies = 20\n",
        "max_iterations = 50\n",
        "gamma = 1.0  # Absorption coefficient\n",
        "alpha = 0.2  # Randomness factor\n",
        "\n",
        "# Scoring function that combines Spearman and Mutual Information scores\n",
        "# Normalize scores between 0 and 1, then sum\n",
        "spearman_scores = spearman_corr / spearman_corr.max()\n",
        "mutual_info_scores = mutual_info_series / mutual_info_series.max()\n",
        "combined_scores = spearman_scores + mutual_info_scores\n",
        "\n",
        "# Initialize fireflies with random subsets of features\n",
        "np.random.seed(42)\n",
        "fireflies = [np.random.choice([0, 1], len(X_train.columns)) for _ in range(num_fireflies)]\n",
        "\n",
        "# Define a function to evaluate firefly's fitness based on selected features\n",
        "def evaluate_fitness(firefly):\n",
        "    selected_features = X_train.columns[firefly == 1]\n",
        "    return combined_scores[selected_features].sum()\n",
        "\n",
        "# Firefly Optimization Loop\n",
        "for iteration in range(max_iterations):\n",
        "    for i in range(num_fireflies):\n",
        "        for j in range(num_fireflies):\n",
        "            if evaluate_fitness(fireflies[j]) > evaluate_fitness(fireflies[i]):\n",
        "                # Update firefly i towards firefly j\n",
        "                fireflies[i] = np.where(\n",
        "                    np.random.rand(len(X_train.columns)) < alpha * np.exp(-gamma * np.sum((fireflies[i] - fireflies[j]) ** 2)),\n",
        "                    fireflies[j],\n",
        "                    fireflies[i]\n",
        "                )\n",
        "\n",
        "    # Optional: Decrease alpha over iterations for reduced randomness\n",
        "    # alpha *= 0.95\n",
        "\n",
        "# Select the best firefly as the optimized feature subset\n",
        "best_firefly = max(fireflies, key=evaluate_fitness)\n",
        "optimized_features = X_train.columns[best_firefly == 1]\n",
        "\n",
        "print(\"Optimized feature subset:\\n\", optimized_features)\n",
        "\n",
        "\n",
        "# Output the final selected features\n",
        "print(\"Final selected features after Firefly Optimization:\\n\", optimized_features)\n"
      ],
      "metadata": {
        "id": "a-YYUX6qF81Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}